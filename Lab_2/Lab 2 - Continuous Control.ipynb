{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66af043",
   "metadata": {},
   "source": [
    "# Continuous control\n",
    "We will now move on and look at some continuous control tasks. Policy gradients are well suited to continuous control tasks as the policy can take the form of some continuous distribution -- the most common is to use either a deterministic policy which maps a state directly to an continuos action, i.e. $\\pi(s) = a$, or to learn the parameters of a Normal distribution. Initially we will look at the Deep Deterministic Policy Gradient (DDPG) algorithm, and then compare this to the Soft Actor-Critic algorithm. \n",
    "\n",
    "Before we do that, let's re-import all the packages and functions we will need that we used in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67509fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, batch_size=128):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def push(self, data):\n",
    "        self.buffer.append(data)\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.buffer, self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "\n",
    "def run_test(alg, env):  # used to run an eval episode for any algorithm with a `greedy_act` method.\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = alg.greedy_act(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        score += reward\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8050090",
   "metadata": {},
   "source": [
    "---\n",
    "# Deep Deterministic Policy Gradient\n",
    "The deterministic policy gradient states that the gradient for a determinstic policy is given by $\\mathbb{E}_{s \\sim \\rho^\\beta}\\left[\\nabla_{\\theta} Q(s, \\pi_\\theta(s)) \\right] = \\mathbb{E}_{s \\sim \\rho^\\beta}\\left[\\nabla_a Q(s, a) \\nabla_{\\theta}\\pi_\\theta(s) \\right]$, where $\\rho^\\beta$ is the state distribution according to some exploratory policy $\\beta$. This is just an application of the chain rule. Thankfully, due to the wonders of autograd, we again don't need to worry about calculating some long gradient via the chain rule, as it will handle it all for us! So to obtain the policy gradient in PyTorch it is as simple as evaluating the policy $a = \\pi(s)$, passing $a$ into the critic $Q$ along with the state, and using `.backward()` as we do with a normal loss. \n",
    "\n",
    "Before we look at implementing the algorithm, we will take a quick look at how the action space changes in a gym environment when it is continuous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f08c140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-2.0, 2.0, (1,), float32)\n",
      "[-1.6669251]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615fae1",
   "metadata": {},
   "source": [
    "So we can see that now the action space is a Box object, similar to the state spaces we have dealt with. In the pendulum environment it is just a one dimensional action in the range $[-2, 2]$. Whilst *most* continuous control benchmark environments have a symmetric action space, they are usually in the range $[-1, 1]$ (just something to note), but they are usually of high dimension that just a single scalar. \n",
    "\n",
    "Let's move on to the code for DDPG! We can use the same replay buffer that we used for the DQN, so we just need to write the code for the actor and critic networks, and then the actual algorithm itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac8a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGActor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size, action_upper_bound):\n",
    "        super(DDPGActor, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_dim)\n",
    "        self.h1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.h2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        self.ub = action_upper_bound\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer.forward(x))\n",
    "        x = torch.relu(self.h1.forward(x))\n",
    "        x = torch.relu(self.h2.forward(x))\n",
    "        x = self.ub * torch.tanh(self.output_layer.forward(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DDPGCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(DDPGCritic, self).__init__()\n",
    "        self.input_layer = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.h1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.h2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat((state, action), dim=1)\n",
    "        x = torch.relu(self.input_layer.forward(x))\n",
    "        x = torch.relu(self.h1.forward(x))\n",
    "        x = torch.relu(self.h2.forward(x))\n",
    "        x = self.output_layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b21164",
   "metadata": {},
   "source": [
    "The actor here is similar to the networks we have seen already, with the exception that the output is passed through a `tanh` activation. This is so that we have a symmetric output in $(-1, 1)$, which we then scale by the upper bound of the environment's action space (in the case of Pendulum this would be 2.0). In general, you don't need to use a `tanh` activation, you could for instance just clamp the outputs to be in between $(-1, 1)$ and then re-scale (test this if you like), but if your action space varies in range, i.e. if your min/max values for each dimension aren't the same, then you are probably better to clamp the values manually to your custom range. The critic is similar to a mix of the PPO and DQN critic we have used prior. In DQN we estimate $Q$ but over a discrete quantity, whereas here the action is continuous so the action must be given as input to the network, of which we will receive a scalar, which is more similar to the PPO critic (though that is a value function, not a $Q$-function). Let's now test out a trained DDPG agent on the Pendulum environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8238c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained DDPG score was -126.32883407238033\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1', render_mode='human')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_upper_bound = env.action_space.high.max()\n",
    "net = DDPGActor(state_dim, 512, action_dim, action_upper_bound)\n",
    "net.load_state_dict(torch.load('networks/Pendulum_ddpg_actor'))\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "while not done:\n",
    "    with torch.no_grad():\n",
    "        state = torch.from_numpy(state).float()\n",
    "        action = net.forward(state).flatten().numpy()  # we should make sure that the action is a numpy array, not a tensor\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    score += reward\n",
    "print(f\"The trained DDPG score was {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2556b9",
   "metadata": {},
   "source": [
    "Let's now have a go at implementing this ourselves. I will provide the entire code for DDPG, **then as an exercise you can have a go at implementing a child class for TD3, an algorithm built on top of DDPG which improves performance. You can follow the pseudocode from the [paper](https://proceedings.mlr.press/v80/fujimoto18a.html), algorithm 1 (if you are pushed for time and would rather look at SAC then come back to this)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac10add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, state_dim, hidden_size, num_actions, action_ub, gamma=0.99, exploration_noise=0.2, memory_size=100000, batch_size=128, tau=0.0025,\n",
    "                 critic_lr=1e-3, actor_lr=1e-4):\n",
    "        self.actor = DDPGActor(state_dim, hidden_size, num_actions, action_ub)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimiser = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "\n",
    "        self.critic = DDPGCritic(state_dim, num_actions, hidden_size)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimiser = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.critic_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "        self.memory = ReplayBuffer(memory_size, batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.num_actions = num_actions\n",
    "        self.exploration_noise = exploration_noise\n",
    "        self.ub = action_ub\n",
    "        self.lb = -self.ub\n",
    "        self.alg_name = 'ddpg'\n",
    "\n",
    "        self.grad_steps = 0\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.push((state.flatten(), action.flatten(), reward, next_state.flatten(), done))\n",
    "\n",
    "    def get_batch(self):\n",
    "        batch = self.memory.sample()\n",
    "\n",
    "        states = torch.from_numpy(np.array([s for s, _, _, _, _ in batch])).float()\n",
    "        actions = torch.from_numpy(np.array([a for _, a, _, _, _ in batch])).float()\n",
    "        rewards = torch.FloatTensor([[r] for _, _, r, _, _ in batch])\n",
    "        next_states = torch.from_numpy(np.array([ns for _, _, _, ns, _ in batch])).float()\n",
    "        dones = torch.FloatTensor([[d] for _, _, _, _, d in batch])\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state)\n",
    "            action = self.actor.forward(state).numpy().flatten()\n",
    "            action += np.random.normal(0, self.exploration_noise, size=self.num_actions)\n",
    "            return np.clip(action, self.lb, self.ub)\n",
    "\n",
    "    def greedy_act(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state)\n",
    "            action = self.actor.forward(state).numpy().flatten()\n",
    "            return np.clip(action, self.lb, self.ub)\n",
    "\n",
    "    def update_target(self):\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.get_batch()\n",
    "\n",
    "        q_vals = self.critic.forward(states, actions)\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.actor_target.forward(next_states)\n",
    "            next_q_vals = self.critic_target.forward(next_states, next_actions)\n",
    "            target = rewards + self.gamma * (1 - dones) * next_q_vals\n",
    "        self.critic_optimiser.zero_grad()\n",
    "        critic_loss = self.critic_loss_fn(q_vals, target)\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimiser.step()\n",
    "\n",
    "        self.actor_optimiser.zero_grad()\n",
    "        actor_loss = -self.critic.forward(states, self.actor.forward(states)).mean()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimiser.step()\n",
    "\n",
    "        self.grad_steps += 1\n",
    "        self.update_target()\n",
    "\n",
    "    def save_model(self, optional_path=\"\"):\n",
    "        actor = copy.deepcopy(self.actor)\n",
    "        critic = copy.deepcopy(self.critic)\n",
    "        torch.save(critic.state_dict(), f\"{optional_path}_{self.alg_name}_critic\")\n",
    "        torch.save(actor.state_dict(), f\"{optional_path}_{self.alg_name}_actor\")\n",
    "\n",
    "    def load_model(self, optional_path=\"\"):\n",
    "        self.actor.load_state_dict(torch.load(f\"{optional_path}_{self.alg_name}_actor\"))\n",
    "        self.critic.load_state_dict(torch.load(f\"{optional_path}_{self.alg_name}_critic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180be35",
   "metadata": {},
   "source": [
    "The class is very similar to the DQN class we made earlier. The major change in the `init` is that we now provide an `exploration_noise` parameter that is the standard dev. of a normal distribution that we sample from and add as exploratory noise. In the `get_batch` method note that now, as our actions are continuous, they are also made into a tensor with the `Float` data-type, rather than `Long`. \n",
    "\n",
    "The critic is trained in a similar way to that which we have seen. Perform a forward pass of our current network for the estimate of the $Q$-value, and then compute the target value using the observed rewards and bootstrapping the value from the next state-action pair, where the next action is generated from our target actor network. The actor loss is very simple to calculate, as mentioned at the start of the section, we simply evaluate the critic at the current state with actions coming from the actor network. As with PPO, **remember to take a negative so that we do have a 'loss' to minismise, since we actually want to maximise the critic values!**.\n",
    "\n",
    "And there we have it, our first continuous control algorithm! Let's see how it performs on the Pendulum environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f28a568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad steps: 1000. Score: -1421.5344259725166.\n",
      "Grad steps: 2000. Score: -1216.0460907242204.\n",
      "Grad steps: 3000. Score: -369.42793752268545.\n",
      "Grad steps: 4000. Score: -175.785608671645.\n",
      "Grad steps: 5000. Score: -108.39953607617625.\n",
      "Grad steps: 6000. Score: -193.68439201631.\n",
      "Grad steps: 7000. Score: -107.26759802689318.\n",
      "Grad steps: 8000. Score: -151.43578176546862.\n",
      "Grad steps: 9000. Score: -149.29236506829002.\n",
      "Grad steps: 10000. Score: -112.66637990201427.\n",
      "Grad steps: 11000. Score: -124.2061166889783.\n",
      "Grad steps: 12000. Score: -112.1826415557251.\n",
      "Grad steps: 13000. Score: -125.03645093694186.\n",
      "Grad steps: 14000. Score: -96.61743854802293.\n",
      "Grad steps: 15000. Score: -145.33077361268525.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "test_env = gym.make('Pendulum-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "action_upper_bound = env.action_space.high.max()\n",
    "ddpg = DDPG(state_dim, 512, num_actions, action_upper_bound, memory_size=100000, batch_size=256)\n",
    "\n",
    "while len(ddpg.memory) < 10_000:\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        ddpg.remember(state, action, reward, next_state, terminated)\n",
    "        state = next_state\n",
    "\n",
    "episode = 0\n",
    "ddpg_test_scores = []\n",
    "while ddpg.grad_steps < 15000:\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    episode += 1\n",
    "    while not done:\n",
    "        action = ddpg.act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        ddpg.remember(state, action, reward, next_state, terminated)\n",
    "        ddpg.experience_replay()\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if ddpg.grad_steps % 1000 == 0:\n",
    "            test_score = []\n",
    "            for test in range(10):\n",
    "                test_score.append(run_test(ddpg, test_env))\n",
    "            ddpg_test_scores.append((ddpg.grad_steps, np.mean(test_score)))\n",
    "            print(f\"Grad steps: {ddpg.grad_steps}. Score: {np.mean(test_score)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be64e0ae",
   "metadata": {},
   "source": [
    "Now you should have a go at writing a child class, in the cell below, for the TD3 algorithm as previously mentioned. There should only be changes to any extra hyperparameters you need in the `init` and the `experience_replay` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa79eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9de187c2",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Soft Actor-Critic\n",
    "Soft Actor-Critic (SAC) is a deep reinforcement learning algorithm that uses a maximum-entropy framework to optimise the policy and the value function simultaneously. We learn the policy parameters by *minimising* $\\mathbb{E}_{s \\sim \\mathcal{B}}\\left[ \\mathbb{E}_{a \\sim \\pi_\\phi}\\left[ \\alpha \\log \\pi_\\phi(a|s) - Q_\\theta(s, a) \\right] \\right]$, where $\\phi$ are the parameters of the policy, $\\theta$ are the parameters of the $Q$-function, $\\alpha$ is the temperature hyper-parameter which balances out how much we care about maximising the entropy, and $\\mathcal{B}$ is our replay buffer. We learn the parameters of the $Q$-function by minimising \n",
    "$$\\mathbb{E}_{(s, a) \\sim \\mathcal{B}} \\left[ \\left(Q_\\theta(s, a) - \\left(r(s, a) + \\gamma \\mathbb{E}_{a'\\sim \\pi_\\phi, s' \\sim p} \\left[Q_{\\theta'}(s', a') - \\alpha \\log \\pi_\\phi(a'|s')\\right] \\right)\\right)^2\\right]\\;;$$\n",
    "where $\\theta'$ are the parameters of the target critic network and $p$ is the transition function of the MDP. \n",
    "\n",
    "In the original paper, they treat $\\alpha$ as a hyperparameter, but in an updated work [(found here)](https://arxiv.org/abs/1812.05905) they provide information on how we can automatically tune it. This is beyond the scope of the lab, but I will provide code on how to do this as part of the implementation!\n",
    "\n",
    "You may have also noticed that we are required to evaluate the critic at an action sampled from the distribution we are learning. This is different to the DDPG case where we directly output the action, because here we are learning the *parameters* of a distribution (e.g. the mean and var of a Normal distn), and so we need to ensure that the sampled action is differentiable with respect to the weights (I say weights here to distinguish between the parameters of the distribution). This is non-trivial to do, and it largely limits us to learning the parameters of distributions which are susceptible to the reparameterisation trick [(this stack exchange answer summarises things well)](https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important). Luckily for us, we will largely be concerned with learning the parameters of a Normal distribution, which can be reparameterised by noting that the random variable $Y = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim N(0, 1)$, has a $N(\\mu, \\sigma^2)$ distribution. So we just need to learn the mean and standard deviation, sample the noise from a unit normal distribution and we can differentiate (w.r.t. our network weights) through our sampled action. \n",
    "\n",
    "Now, the final thing to note is that the support of the Normal distribution is entire real line, whereas our action space for the Pendulum environment was $[-2, 2]$. So, how do we deal with this? We use a transformation! If we learn the parameters of a normal distribution, then we can squash it to be in our desired range by using a $\\mbox{tanh}$ transformation. That is, if we have $X \\sim N(\\mu, \\sigma^2)$, then the transformed random variable $Y = a \\times \\mbox{tanh}(X)$ (where in the case of pendulum $a=2$) will have support in our desired range. You can calculate the pdf of $Y$ analytical, if you like (it will be easiest for the univariate case), by using the fact that $f_Y(y) = f_X(x) \\left|\\frac{dx}{dy}\\right|$, evaluated at $x = g^{-1}(y)$ (where $g$ in this case is $\\mbox{tanh}$), but you might be relieved to know that PyTorch has built in functionality for transformations of random variables so we don't need to worry implementing this ourselves -- we'll show a quick example of this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59222c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4328,  4.3316, 10.5660, 38.5486]])\n",
      "tensor([[-1.4290,  1.9998,  2.0000,  2.0000]])\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.transformed_distribution import TransformedDistribution\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "from torch.distributions.independent import Independent\n",
    "from torch.distributions.transforms import AffineTransform\n",
    "\n",
    "means = torch.FloatTensor([[0, 5, 10, 40]])\n",
    "variances = torch.ones_like(means)  # vars need to be positive\n",
    "normal_distribution = Independent(Normal(means, variances), 1)  # we typically will assume indepdence between the different dimensions of the action space\n",
    "print(normal_distribution.sample())  # sample from the distribution\n",
    "\n",
    "transformed_distribution = TransformedDistribution(Independent(Normal(means, variances), 1), [TanhTransform(cache_size=1),\n",
    "                                                                                                AffineTransform(0, 2, cache_size=1)])\n",
    "print(transformed_distribution.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f9c6d",
   "metadata": {},
   "source": [
    "The initial distribution was 4 independent normals with means 0, 5, 10, 40 and each with variance of 1. We can see that when we sample from them we have numbers we would expect, all centred roughly around their means. In the transformed distribution we can see they have a max value of 2 (they will also have a minimum value of 2), which we would expect from the transformed distribution. The `TransformedDistribution` function takes in an original distribution, followed by a list of transformations to apply, in the order they are given. We first give the `TanhTransform` transformation, which applies the $\\mbox{tanh}$ transformation, followed  by an affine transformation -- in this case we shifted by 0 and scaled by 2. If you're lucky enough to work on problems with an action space that has values in $[-1, 1]^d$, then you will not need to use the `AffineTransform`. \n",
    "\n",
    "Now that we have seen how to transform the distribution, let's get on with writing the code for the algorithm. I will provide most of the code for you, including the automatic tuning of $\\alpha$ and where to use it, but I will leave the optimisation of the policy and $Q$ for you to fill in. First, we need the Actor and Critic networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "177efbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACActor(nn.Module):\n",
    "    def __init__(self, input_dim, num_actions, hidden_size=256, sd_min=-20, sd_max=10):\n",
    "        super(SACActor, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_size)\n",
    "        self.h1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.mean_layer = nn.Linear(hidden_size, num_actions)\n",
    "        self.sd_layer = nn.Linear(hidden_size, num_actions)\n",
    "\n",
    "        self.log_sd_min = sd_min\n",
    "        self.log_sd_max = sd_max\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.input_layer(state))\n",
    "        x = torch.relu(self.h1(x))\n",
    "        x = torch.relu(self.h2(x))\n",
    "        mean = self.mean_layer(x)\n",
    "        log_sd = self.sd_layer(x)\n",
    "        log_sd = log_sd.clamp(self.log_sd_min, self.log_sd_max)\n",
    "        return mean, log_sd\n",
    "\n",
    "class SACCritic(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, action_dim):\n",
    "        super(SACCritic, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim + action_dim, hidden_size)\n",
    "        self.h1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.input_layer2 = nn.Linear(input_dim + action_dim, hidden_size)\n",
    "        self.h21 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h22 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x_ = torch.cat((state, action), dim=1)\n",
    "        x = torch.relu(self.input_layer(x_))\n",
    "        x = torch.relu(self.h1(x))\n",
    "        x = torch.relu(self.h2(x))\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        x1 = torch.relu(self.input_layer2(x_))\n",
    "        x1 = torch.relu(self.h21(x1))\n",
    "        x1 = torch.relu(self.h22(x1))\n",
    "        x1 = self.output_layer2(x1)\n",
    "        return x, x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeaa1d0",
   "metadata": {},
   "source": [
    "The actor is similar to the PPO actor, but we now have two output layer, one for the mean and the other for the (log) standard deviation. We clamp the `log_sd` to be in a certain range. This ensures that it does not tend to 0 (no exploration) or be too big (too much exploration). The critic is similar to that of the DDPG, taking in an action and a state, but you'll note that we essentially maintain two critics. This is because to try and ovecome maximisation bias, in the policy update and critic update we take the minimum of the two critics. \n",
    "\n",
    "Onto the SAC class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341fc5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self, state_dim, num_actions, hidden_size=256, gamma=0.99, batch_size=256, max_memory=100000, tau=0.0025, critic_lr=1e-3, actor_lr=1e-4, action_scale=1):\n",
    "\n",
    "        self.actor = SACActor(state_dim, hidden_size, num_actions)\n",
    "        self.actor_optimiser = Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic = SACCritic(state_dim, hidden_size, num_actions)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimiser = Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.alg_name = \"SAC\"\n",
    "\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device='cpu')\n",
    "        self.alpha_optimiser = Adam([self.log_alpha], lr=critic_lr)\n",
    "\n",
    "        self.memory = ReplayBuffer(max_memory, batch_size)\n",
    "        self.action_scale = action_scale\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.critic_loss_function = nn.MSELoss()\n",
    "        self.target_entropy = -num_actions\n",
    "        self.tau = tau\n",
    "        self.grad_steps = 0\n",
    "        self.actor_loss = deque(maxlen=10000)\n",
    "        self.critic_loss = deque(maxlen=10000)\n",
    "        self.alpha_loss = deque(maxlen=10000)\n",
    "        self.alpha_gradient = []\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def get_batch(self):\n",
    "        batch = self.memory.sample()\n",
    "\n",
    "        states = torch.from_numpy(np.array([s for s, _, _, _, _ in batch])).float()\n",
    "        actions = torch.from_numpy(np.array([a for _, a, _, _, _ in batch])).float()\n",
    "        rewards = torch.FloatTensor([[r] for _, _, r, _, _ in batch])\n",
    "        next_states = torch.from_numpy(np.array([ns for _, _, _, ns, _ in batch])).float()\n",
    "        dones = torch.FloatTensor([[d] for _, _, _, _, d in batch])\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        action = action.flatten()\n",
    "        self.memory.push((state, action, reward, next_state, done))\n",
    "\n",
    "    def update_target(self):\n",
    "        for real, target in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target.data.copy_(real.data * self.tau + target.data * (1 - self.tau))\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < self.batch_size * 10:\n",
    "            return\n",
    "        states, old_actions, rewards, next_states, dones = self.get_batch()\n",
    "\n",
    "        alpha = self.log_alpha.exp().detach()\n",
    "\n",
    "        # critic loss\n",
    "        # fill in here\n",
    "\n",
    "        self.critic_optimiser.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimiser.step()\n",
    "\n",
    "        # actor loss\n",
    "        # fill in here\n",
    "\n",
    "        alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()\n",
    "        self.alpha_loss.append(alpha_loss.item())\n",
    "\n",
    "        self.actor_optimiser.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimiser.step()\n",
    "\n",
    "        self.alpha_optimiser.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimiser.step()\n",
    "\n",
    "        self.update_target()\n",
    "        self.grad_steps += 1\n",
    "\n",
    "    def get_actions_log_probs(self, states):\n",
    "        dist = self.get_distribution(states)\n",
    "        actions = dist.rsample()\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        return actions, log_probs.unsqueeze(dim=-1)\n",
    "\n",
    "    def get_distribution(self, states):\n",
    "        mean, log_sd = self.actor.forward(states)\n",
    "        tanh_dist = TransformedDistribution(Independent(Normal(mean, log_sd.exp()), 1), [TanhTransform(cache_size=1),\n",
    "                                                                                         AffineTransform(0, self.action_scale, cache_size=1)])\n",
    "        return tanh_dist\n",
    "\n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(dim=0)\n",
    "            dist = self.get_distribution(state)\n",
    "            action = dist.rsample()\n",
    "            return action.numpy().flatten()\n",
    "\n",
    "    def greedy_act(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(dim=0)\n",
    "            mean, log_sd = self.actor.forward(state)\n",
    "            mean = torch.tanh(mean)\n",
    "            return mean.numpy().flatten()\n",
    "\n",
    "    def save_model(self, optional_path=\"\"):\n",
    "        actor = copy.deepcopy(self.actor)\n",
    "        critic = copy.deepcopy(self.critic)\n",
    "        torch.save(critic.state_dict(), f\"{optional_path}_{self.alg_name}_critic\")\n",
    "        torch.save(actor.state_dict(), f\"{optional_path}_{self.alg_name}_actor\")\n",
    "\n",
    "    def load_model(self, optional_path=\"\"):\n",
    "        self.actor.load_state_dict(torch.load(f\"{optional_path}_{self.alg_name}_actor\"))\n",
    "        self.critic.load_state_dict(torch.load(f\"{optional_path}_{self.alg_name}_critic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1200899",
   "metadata": {},
   "source": [
    "I've left the critic loss and actor loss for you to complete yourselves (any questions, please ask). You will want to make use of the `get_action_log_probs` method. It will take in a state (or batch of states), perform a forward pass of the network to get the parameters of the distribution, and then sample from the distribution, returning those sampled actions and the corresponding log-probabilities for those actions. **Note:** in PPO we saw that we can sample from a distribution using `.sample()`. This is fine if we only want the action, but the action returned from this *will not be differentiable*. To obtain a differentiable action, we need to sample using `.rsample()`, which is what we use in the aforementioned method. Some PyTorch distributions will not have `.rsample()` implemented if the reparameterisation trick cannot be used.\n",
    "\n",
    "Now, run your implementation of SAC with the below code and then compare to DDPG, like with did with PPO and DQN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "test_env = gym.make('Pendulum-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "action_upper_bound = env.action_space.high.max()\n",
    "sac = SAC(state_dim, num_actions, 512, action_scale=action_upper_bound, max_memory=100000, batch_size=256)\n",
    "\n",
    "while len(sac.memory) < 10_000:\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        sac.remember(state, action, reward, next_state, terminated)\n",
    "        state = next_state\n",
    "\n",
    "episode = 0\n",
    "sac_test_scores = []\n",
    "while sac.grad_steps < 15000:\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    episode += 1\n",
    "    while not done:\n",
    "        action = sac.act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        sac.remember(state, action, reward, next_state, terminated)\n",
    "        sac.experience_replay()\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if sac.grad_steps % 1000 == 0:\n",
    "            test_score = []\n",
    "            for test in range(10):\n",
    "                test_score.append(run_test(sac, test_env))\n",
    "            sac_test_scores.append((sac.grad_steps, np.mean(test_scores)))\n",
    "            print(f\"Grad steps: {sac.grad_steps}. Score: {np.mean(test_score)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a7c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([x[0] for x in sac_test_scores], [x[1] for x in sac_test_scores], label='SAC')\n",
    "plt.plot([x[0] for x in ddpg_test_scores], [x[1] for x in ddpg_test_scores], label='DDPG')\n",
    "# plt.plot([x[0] for x in TD3_test_scores], [x[1] for x in TD3_test_scores], label='TD3')  # uncomment if you were able to implement TD3\n",
    "plt.legend()\n",
    "plt.ylabel(\"Test Score\")\n",
    "plt.xlabel(\"Grad Steps\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
