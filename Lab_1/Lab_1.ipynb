{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77a5fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncomment here if you need to install the relevant packages\n",
    "# !pip install gymnasium \n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0155d3",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Lab 1: Tabular RL\n",
    "---\n",
    "In this lab session we will aim to accomplish the following:\n",
    "- An introduction to [Gym(nasium)](https://gymnasium.farama.org); a commonly used package with benchmark RL environments\n",
    "- Run and compare Monte-Carlo and Temporal Difference based algorithms;\n",
    "- Write code that visualises the results of training the various algorithms.\n",
    "\n",
    "Note that I've tried to highlight in **bold** parts of the lab that you can do yourself, or questions that you can discuss with the person sitting next to you, or ask me about in the lab. \n",
    "\n",
    "### First, we will begin by installing Gym and looking at it's functionality. \n",
    "Gym is an important framework in RL because it provides a standardised interface for interacting with RL environments, allowing researchers and practitioners to easily compare the performance of different RL algorithms on a wide range of environments. It also provides a collection of diverse and challenging environments that can be used for benchmarking and testing new algorithms.\n",
    "\n",
    "Now, let's take a look at how a gym environment works! We will start by creating a copy of a Blackjack environment. For any regular Blackjack players, information about the specific rules of this Blackjack game can be found in the [documentation](https://gymnasium.farama.org/environments/toy_text/blackjack/), with a major difference between this implementation and that of a real casino is that the cards are drawn from an infinite deck. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90386329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b80fcd",
   "metadata": {},
   "source": [
    "As we can see, to create a copy of an environment we use the command `gym.make()` with a string of the environment name that we'd like to create. For more environment names that can be used, you can look at the Gym documentation that was linked above. The format is usually *env_name*-*version*, so be careful to specify the most recent version -- this again can be found in the documentation; for example, if we go to the documentation for the [Lunar Lander environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/) we can scroll down to see the version history. \n",
    "\n",
    "The environment object provides us with information about the environment, most importantly about the state and action space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14d5460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f28bc0",
   "metadata": {},
   "source": [
    "From the output, we can see that the observation space is a 3-tuple. Here, `Discrete(N)` simply means that we have `N` discrete options. For the state space, this corresponds to the players current sum, the value of the dealers visible card, and whether or not the player holds a useable ace, respectively. The action space is simply whether to hit or stick. The rewards for the game are +1 for winning, +1.5 for winning with a 'natural' blackjack, -1 for losing and 0 for a draw. The episode ends whenever a player sticks, when a player hits and the sum of the cards exceeds 21, or if the player has blackjack. \n",
    "\n",
    "Now that we have the environment set up, we can play a game using a random policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9729bda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random policy got an average score of -0.4051\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(3)\n",
    "random_blackjack_scores = []\n",
    "for episode in range(10000):\n",
    "    state, _ = env.reset()  # this is how we reset the environment.\n",
    "    done = False  # this tells us that the game is not over yet.\n",
    "    score = 0  # to keep track of the rewards during the episode.\n",
    "    while not done: \n",
    "        action = env.action_space.sample()  # this samples an action uniformly at random from the action space.\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)  # here we take a step in the env using the sampled action.\n",
    "        score += reward  # add on the observed reward.\n",
    "        done = terminated or truncated  # tells us whether the episode is over.\n",
    "        state = next_state  # here we reset the current state variable to be the next state from the previous transition.\n",
    "    random_blackjack_scores.append(score)\n",
    "print(f\"The random policy got an average score of {np.mean(random_blackjack_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc5ba24",
   "metadata": {},
   "source": [
    "The above loop is something you will see a lot whilst learning Reinforcement Learning, and is why the gym framework is a valuable asset as it allows us a simple, clean way of looping through episodes of an environment. Note that when we reset the environment, `env.reset()` will actually return two objects -- the `state` and `info` (which we assign to an unnamed variable). The `info` is there to tell us if there are any problems with the environment or any bits of information that may be useful, and is only really used for debugging if something is going wrong. It is also returned when calling `env.step()`, but for now we don't need to worry about it. \n",
    "\n",
    "The other thing to note is that we are returned a `terminated` and a `truncated` flag by the environment when we take a step. This is not something we need to worry about for Blackjack, but in continuing problems (i.e. those with no terminal states), or those with very long time horizons, it can be useful to reset the environment whilst training before an episode ends. However, we don't want to treat this as a terminal state, and so the `terminated` flag is what we use to tell the algorithm whether or not the state is terminal, whereas the `truncated` flag is for our use in training to tell us whether or not we should reset the environment, so during training we now have two reset conditions: whether the state was truly terminal, or whether we are truncating the episode. \n",
    "\n",
    "Next we'll move on to Mote Carlo methods and see if we can beat the random policy. \n",
    "\n",
    "--- \n",
    "\n",
    "## Monte Carlo Control\n",
    "We'll now focus on some Monte Carlo based algorithms. Monte Carlo Control is a family of RL algorithms that learns to find an optimal policy by repeatedly playing episodes of a given environment and estimating the expected return of each state-action pair based on the observed returns from those pairs in the played episodes, i.e. using a Monte Carlo approach to estimate the expected returns.\n",
    "\n",
    "Below I have provided a class that implements an algorithm based on the first visit Monte Carlo estimates of the returns. First you should read through the code to see if you can understand it for yourself, and then I will add details of each method below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6220ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Tuple\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "class MonteCarloControl:\n",
    "    def __init__(self, env, num_episodes=10000, gamma=0.99, epsilon=0.3):\n",
    "        self.env = env\n",
    "        self.num_episodes = num_episodes\n",
    "        self.gamma = gamma\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.Q = {}\n",
    "        self.N = {}\n",
    "        if type(env.observation_space) is Tuple:\n",
    "            self.num_states = np.prod([a.n for a in env.observation_space])\n",
    "            self.possible_states = [range(a.n) for a in env.observation_space]\n",
    "            for state in product(*self.possible_states):\n",
    "                self.Q[state] = np.random.uniform(-0.1, 0.1, self.num_actions)\n",
    "                self.N[state] = np.zeros(self.num_actions)\n",
    "                self.returns = {(s, a): [] for s in product(*self.possible_states) for a in range(self.num_actions)}\n",
    "        else:\n",
    "            self.possible_states = range(env.observation_space.n)\n",
    "            self.num_states = env.observation_space.n\n",
    "            for state in self.possible_states:\n",
    "                self.Q[state] = np.random.uniform(-0.1, 0.1, self.num_actions)\n",
    "                self.N[state] = np.zeros(self.num_actions)\n",
    "                self.returns = {(s, a): [] for s in self.possible_states for a in range(self.num_actions)}\n",
    "        self.policy = self.create_random_policy()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def create_random_policy(self):\n",
    "        policy = {}\n",
    "        for s in self.Q:\n",
    "            policy[s] = np.random.dirichlet(np.ones(self.num_actions), size=1).flatten()\n",
    "        return policy\n",
    "\n",
    "    def get_action(self, state):\n",
    "        probs = self.policy[state]\n",
    "        action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "        return action\n",
    "\n",
    "    def generate_episode(self):\n",
    "        episode = []\n",
    "        state, _ = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        return episode\n",
    "\n",
    "    def update_Q(self, episode):\n",
    "        G = 0\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = self.gamma * G + reward\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:\n",
    "                self.N[state][action] += 1\n",
    "                self.Q[state][action] += 1 / self.N[state][action] * (G - self.Q[state][action])\n",
    "                self.policy[state] = np.zeros(self.num_actions)\n",
    "                best_action = np.argmax(self.Q[state])\n",
    "                self.policy[state] = np.array([self.epsilon / self.num_actions] * self.num_actions)\n",
    "                self.policy[state][best_action] = 1 - self.epsilon + self.epsilon / self.num_actions\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            if (episode + 1) % 10000 == 0:\n",
    "                print(f\"Starting episode {episode + 1}\")  # comment out if you don't want to have the progress printed\n",
    "            episode = self.generate_episode()\n",
    "            self.update_Q(episode)\n",
    "\n",
    "    def generate_test_episode(self):\n",
    "        state, _ = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b33c2",
   "metadata": {},
   "source": [
    "In the `init` we pass in the environment, how many episodes we'd like to train for, the discount factor ($\\gamma$) and our $\\epsilon$ parameter for the soft policy. Recall that a soft policy is one which assigns probability $\\frac{\\epsilon}{|\\mathcal{A}|}$ to all non-greedy actions, and the remaining $1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}|}$ probability to the greedy value. Using the environment, we loop through all possible states and initialise our Q-table, `self.Q` and our visitation count `self.N`. We then also initialise the policy randomly using the `create_random_policy` method. \n",
    "\n",
    "Next, we have two straight forward methods: `get_action` and `generate_episode`. The former simply returns an action by sampling from the policy, whilst the latter runs through an episode similar to our above example, except here we append the `(state, action, reward)` tuple that we will use to update our Q-table. \n",
    "\n",
    "As the name suggests, the `update_Q` method is where we will use the episode information to update our Q-table. We receive an `episode` object, which is just a list of the tuples from the most recent episode. Using the list, we update Q using the first-visit Monte Carlo method, which will store the returns we saw from the *first* time we visited a state-action pair in the episode. \n",
    "\n",
    "To be more efficient, we do this dynamically using the following update: `NewEstimate = OldEstimate + 1/N * (NewValue - OldEstimate)`, where `NewValue` is the new return value we observe and `OldEstimate` is our previous estimate of the returns. Once we have update our Q-table, we then update our policy to be $\\epsilon$-soft with respect to the new Q-Values. \n",
    "\n",
    "And there we have our first RL algorithm! Let's see if it can beat the random policy that we tested with earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7dfed62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 10000\n",
      "Starting episode 20000\n",
      "Starting episode 30000\n",
      "Starting episode 40000\n",
      "Starting episode 50000\n",
      "Starting episode 60000\n",
      "Starting episode 70000\n",
      "Starting episode 80000\n",
      "Starting episode 90000\n",
      "Starting episode 100000\n",
      "Average test score was -0.0847\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "env = gym.make('Blackjack-v1')\n",
    "mc = MonteCarloControl(env, num_episodes=100000, epsilon=0.1)  # try playing around with epsilon if you like, but be careful not to go too high on the number of episodes else it will take a while to run\n",
    "mc.train()\n",
    "mc_test_scores = []\n",
    "for ep in range(10000):\n",
    "    mc_test_scores.append(mc.generate_test_episode())\n",
    "print(f\"Average test score was {np.mean(mc_test_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d2874",
   "metadata": {},
   "source": [
    "We can see that the average test score from 100,000 training episodes is about -0.1. This is an improvement over the random policy, but sadly you will still lose your money if you take this policy to the casino ðŸ˜Š\n",
    "\n",
    "If you want to, you can change the environment to another of the environments from the Toy Text section of Gym and see how well the Monte Carlo algorithm fares (be careful not to choose any algorithms that truncate an episode!). \n",
    "\n",
    "---\n",
    "## Temporal Difference Learning\n",
    "We will now take a look at Temporal Difference (TD) Learning. One of the downsides of Monte Carlo learning is that we have to wait *until the end of an episode* before we receive any feedback about our actions. What if we don't want to wait this long? What if we are in a continuing task where there is no end to an episode? That is where TD Learning comes in, as rather than wait until the end of an episode to calculate the Monte Carlo returns, we simply bootstrap the remaining returns from our current estimate. The simplest example is the SARSA algorithm which makes the following update: $Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r(s, a) + \\gamma Q(s', a') - Q(s, a) \\right]$, where $a'$ is an action which we sample from our current policy $\\pi$, i.e. $a' \\sim \\pi(\\cdot|s')$. We can see that our returns are calculated by bootstrapping the value from the next state $s'$, rather than waiting for the end of the episode.\n",
    "\n",
    "Let's write some code for this and see how it compares to the Monte-Carlo method. As before, I will provide the code for you to look through to understand for yourself and then you can read my comments below on which each method is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ac3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    def __init__(self, env, alpha, gamma, epsilon, num_episodes=100000):\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.Q = {}\n",
    "        if type(env.observation_space) is Tuple:\n",
    "            self.num_states = np.prod([a.n for a in env.observation_space])\n",
    "            self.possible_states = [range(a.n) for a in env.observation_space]\n",
    "            for state in product(*self.possible_states):\n",
    "                self.Q[state] = np.random.uniform(-0.1, 0.1, self.num_actions)\n",
    "        else:\n",
    "            self.possible_states = range(env.observation_space.n)\n",
    "            self.num_states = env.observation_space.n\n",
    "            for state in self.possible_states:\n",
    "                self.Q[state] = np.random.uniform(-0.1, 0.1, self.num_actions)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = self.env.action_space.sample()  # explore\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state])  # exploit\n",
    "        return action\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state, next_action, terminated):\n",
    "        td_target = reward + self.gamma * self.Q[next_state][next_action] * (1 - terminated)\n",
    "        td_error = td_target - self.Q[state][action]\n",
    "        self.Q[state][action] += self.alpha * td_error\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            if (episode + 1) % 10000 == 0:\n",
    "                print(f\"Starting episode {episode + 1}\")\n",
    "            state, _ = self.env.reset()\n",
    "            action = self.choose_action(state)\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                next_action = self.choose_action(next_state)\n",
    "                self.update_Q(state, action, reward, next_state, next_action, terminated)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "    def generate_test_episode(self):\n",
    "        state, _ = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action = self.get_greedy_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        return score\n",
    "\n",
    "    def get_greedy_action(self, state):\n",
    "        return np.argmax(self.Q[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a144fd",
   "metadata": {},
   "source": [
    "The code follows the general class structure as the Monte Carlo method (in general, it is good practice to find a layout for your classes that you like and stick to it, so that any algorithms you write and publish are more readable). The only real changes are in the `train` method and the `update_Q` method. In the latter, we have just changed our update rule to be that of the SARSA algorithm. In the former, we have changed slightly the order of things -- now we take an action with the initial state, and pass in the next state to the `update_Q` method. Other than that, things are pretty much as they were!\n",
    "\n",
    "Let's see how SARSA compared to the Monte-Carlo algorithm in Blackjack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66e21d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 10000\n",
      "Starting episode 20000\n",
      "Starting episode 30000\n",
      "Starting episode 40000\n",
      "Starting episode 50000\n",
      "Starting episode 60000\n",
      "Starting episode 70000\n",
      "Starting episode 80000\n",
      "Starting episode 90000\n",
      "Starting episode 100000\n",
      "SARSA test score was -0.0584\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "env = gym.make('Blackjack-v1')\n",
    "sarsa = SARSA(env, 0.8, 0.99, 0.05, 100000)  # try playing around with the hyper-parameters to see if you can improve the score\n",
    "sarsa.train()\n",
    "sarsa_test_scores = []\n",
    "for ep in range(10000):\n",
    "    sarsa_test_scores.append(sarsa.generate_test_episode())\n",
    "print(f\"SARSA test score was {np.mean(sarsa_test_scores)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb3c43",
   "metadata": {},
   "source": [
    "On the seed we tested on, SARSA gets a similar score to the Monte-Carlo algorithm. **Would you have expected this in an environment like Blackjack? Why or why not?**\n",
    "\n",
    "### Q-Learning\n",
    "We are now going to test our first off-policy algorithm, Q-Learning. This algorithm was seminal to making *deep* RL successful, and we will look at the deep variant in our next lab, but for now we will stick with the tabular version. The update rule is similar to SARSA: $Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r(s, a) + \\gamma \\max_{a'}Q(s', a') - Q(s, a) \\right]$. We still choose actions according to some exploratory policy, but the value we bootstrap from comes from the greedy policy. **Why is Q-Learning considered an off-policy algorithm?**\n",
    "\n",
    "To write code for Q-Learning, we need to make minimal modifications to the SARSA algorithm. **Below I've attached a child class of the SARSA algorithm and I'll leave it as an exercise to make the changes.** Once you have made the changes, run the Q-Learning algorithm and see how it performs compared to Monte-Carlo and SARSA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e7d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning(SARSA):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(QLearning, self).__init__(**kwargs)\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state, terminated):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1')\n",
    "q_learning = QLearning(env=env, alpha=0.8, gamma=0.99, epsilon=0.05, num_episodes=100000)  # again, try and play with the hyper-parameters\n",
    "q_learning.train()\n",
    "q_learning_test_scores = []\n",
    "for ep in range(10000):\n",
    "    q_learning_test_scores.append(q_learning.generate_test_episode())\n",
    "print(f\"Q-Learning test score was {np.mean(q_learning_test_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
