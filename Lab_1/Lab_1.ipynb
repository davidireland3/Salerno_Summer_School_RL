{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77a5fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncomment here if you need to install the relevant packages\n",
    "# !pip install gymnasium \n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0155d3",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Lab 1: Tabular RL\n",
    "---\n",
    "In this lab session we will aim to accomplish the following:\n",
    "- An introduction to [Gym(nasium)](https://gymnasium.farama.org); a commonly used package with benchmark RL environments;\n",
    "- Look at exact Dynamic Programming methods;\n",
    "- Run and compare Monte-Carlo and Temporal Difference based algorithms;\n",
    "- Write code that visualises the results of training the various algorithms.\n",
    "\n",
    "Note that I've tried to highlight in **bold** parts of the lab that you can do yourself, or questions that you can discuss with the person sitting next to you, or ask me about in the lab. \n",
    "\n",
    "### First, we will begin by installing Gym and looking at it's functionality. \n",
    "Gym is an important framework in RL because it provides a standardised interface for interacting with RL environments, allowing researchers and practitioners to easily compare the performance of different RL algorithms on a wide range of environments. It also provides a collection of diverse and challenging environments that can be used for benchmarking and testing new algorithms.\n",
    "\n",
    "Now, let's take a look at how a gym environment works! We will start by creating a copy of a Blackjack environment. For any regular Blackjack players, information about the specific rules of this Blackjack game can be found in the [documentation](https://gymnasium.farama.org/environments/toy_text/blackjack/), with a major difference between this implementation and that of a real casino is that the cards are drawn from an infinite deck. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90386329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b80fcd",
   "metadata": {},
   "source": [
    "As we can see, to create a copy of an environment we use the command `gym.make()` with a string of the environment name that we'd like to create. For more environment names that can be used, you can look at the Gym documentation that was linked above. The format is usually *env_name*-*version*, so be careful to specify the most recent version -- this again can be found in the documentation; for example, if we go to the documentation for the [Lunar Lander environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/) we can scroll down to see the version history. \n",
    "\n",
    "The environment object provides us with information about the environment, most importantly about the state and action space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d5460",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f28bc0",
   "metadata": {},
   "source": [
    "From the output, we can see that the observation space is a 3-tuple. Here, `Discrete(N)` simply means that we have `N` discrete options. For the state space, this corresponds to the players current sum, the value of the dealers visible card, and whether or not the player holds a useable ace, respectively. The action space is simply whether to hit or stick. The rewards for the game are +1 for winning, +1.5 for winning with a 'natural' blackjack, -1 for losing and 0 for a draw. The episode ends whenever a player sticks, when a player hits and the sum of the cards exceeds 21, or if the player has blackjack. \n",
    "\n",
    "Now that we have the environment set up, we can play a game using a random policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9729bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(3)\n",
    "random_blackjack_scores = []\n",
    "for episode in range(10000):\n",
    "    state, _ = env.reset()  # this is how we reset the environment.\n",
    "    done = False  # this tells us that the game is not over yet.\n",
    "    score = 0  # to keep track of the rewards during the episode.\n",
    "    while not done: \n",
    "        action = env.action_space.sample()  # this samples an action uniformly at random from the action space.\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)  # here we take a step in the env using the sampled action.\n",
    "        score += reward  # add on the observed reward.\n",
    "        done = terminated or truncated  # tells us whether the episode is over.\n",
    "        state = next_state  # here we reset the current state variable to be the next state from the previous transition.\n",
    "    random_blackjack_scores.append(score)\n",
    "print(f\"The random policy got an average score of {np.mean(random_blackjack_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc5ba24",
   "metadata": {},
   "source": [
    "The above loop is something you will see a lot whilst learning Reinforcement Learning, and is why the gym framework is a valuable asset as it provides a simple, clean way of looping through episodes of an environment. Note that when we reset the environment, `env.reset()` will actually return two objects -- the `state` and `info` (which we assign to an unnamed variable). The `info` is there to tell us if there are any problems with the environment or any bits of information that may be useful, and is only really used for debugging if something is going wrong. It is also returned when calling `env.step()`, but for now we don't need to worry about it. \n",
    "\n",
    "The other thing to note is that we are returned a `terminated` and a `truncated` flag by the environment when we take a step. This is not something we need to worry about for Blackjack, but in continuing problems (i.e. those with no terminal states), or those with very long time horizons, it can be useful to reset the environment whilst training before an episode ends. However, we don't want to treat this as a terminal state, and so the `terminated` flag is what we use to tell the algorithm whether or not the state is terminal, whereas the `truncated` flag is for our use in training to tell us whether or not we should reset the environment, so during training we now have two reset conditions: whether the state was truly terminal, or whether we are truncating the episode. \n",
    "\n",
    "Now let's have a look at implementing our own environments. This is useful for research purposes as you may want to make your own toy environment for ablation studies etc. Today we will write a simple gridworld environment (with some variants) that we will test some tabular algorithms on later. \n",
    "\n",
    "Gridworld is a classic environment where we have an $N \\times M$ grid (usually $N = M$) and we have to navigate from some starting state to a goal state taking the primitive directions as actions (up/down/left/right). It is a useful environment because we can control the difficulty by e.g. introducing obstacles, or having the goal state changing constantly, but for now we will keep it simple. Let's look at the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa2b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class Gridworld(gym.Env):\n",
    "    def __init__(self, grid_size):\n",
    "        super(Gridworld, self).__init__()\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.num_rows = grid_size\n",
    "        self.num_cols = grid_size\n",
    "        self.action_space = spaces.Discrete(4)  # Up, Down, Left, Right\n",
    "        self.observation_space = spaces.Discrete(grid_size * grid_size)\n",
    "\n",
    "        self.start_state = 0\n",
    "        self.goal_state = grid_size * grid_size - 1\n",
    "        self.current_state = self.start_state\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self._state_to_position(self.current_state)\n",
    "\n",
    "        if action == 0:  # Up\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 1:  # Down\n",
    "            x = min(self.grid_size - 1, x + 1)\n",
    "        elif action == 2:  # Left\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 3:  # Right\n",
    "            y = min(self.grid_size - 1, y + 1)\n",
    "\n",
    "        new_state = self._position_to_state(x, y)\n",
    "        self.current_state = new_state\n",
    "\n",
    "        done = False\n",
    "        reward = -1\n",
    "\n",
    "        if self.current_state == self.goal_state:\n",
    "            done = True\n",
    "\n",
    "        return new_state, reward, done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.zeros((self.grid_size, self.grid_size))\n",
    "        x, y = self._state_to_position(self.current_state)\n",
    "        grid[x][y] = 1\n",
    "        print(grid)\n",
    "\n",
    "    def _state_to_position(self, state):\n",
    "        x = state // self.grid_size\n",
    "        y = state % self.grid_size\n",
    "        return x, y\n",
    "\n",
    "    def _position_to_state(self, x, y):\n",
    "        return x * self.grid_size + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f968f",
   "metadata": {},
   "source": [
    "We can see that the class inherits from `gym.Env` (this is useful if you want to use your environment with Stable Baselines, where they expect a gym environment to be specified) and we specify the state(/observation) and action space. We also provide the `reset` and `step` methods, which as we saw with the Blackjack example are how we control the environment. We also added the `render` method which will print out the current grid with the agents location. Next, we will write code for a 'windy' gridworld, where in the middle of the grid there is a wind that will blow all transitions upwards by an extra square, and a cliff walking gridworld where the agent starts in the bottom left of the grid, and must reach the bottom right, but the rest of the bottom row is a cliff that if the agent steps onto will result in the end of an episode with a reward of -100. These environments are taken from Example 6.5 and 6.6 of the Sutton and Barto textbook, respectively. Note that these two classes have a `max_steps` attribute -- this will truncate all episodes once they reach this many steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f36724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindyGridworld(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(WindyGridworld, self).__init__()\n",
    "\n",
    "        self.grid_size = (7, 10)\n",
    "        self.action_space = spaces.Discrete(4)  # Up, Down, Left, Right\n",
    "        self.observation_space = spaces.Discrete(self.grid_size[0] * self.grid_size[1])\n",
    "\n",
    "        self.start_state = (3, 0)\n",
    "        self.goal_state = (3, 7)\n",
    "        self.current_state = self.start_state\n",
    "        self.wind = [0, 0, 0, 1, 1, 1, 1, 1, 1, 0]\n",
    "        self.max_steps = 500\n",
    "        self.steps = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.start_state\n",
    "        self.steps = 0\n",
    "        return self._state_to_index(self.current_state), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        x, y = self.current_state\n",
    "\n",
    "        if action == 0:  # Up\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 1:  # Down\n",
    "            x = min(self.grid_size[0] - 1, x + 1)\n",
    "        elif action == 2:  # Left\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 3:  # Right\n",
    "            y = min(self.grid_size[1] - 1, y + 1)\n",
    "\n",
    "        x = max(0, x - self.wind[y])  # apply the wind\n",
    "\n",
    "        new_state = (x, y)\n",
    "        self.current_state = new_state\n",
    "\n",
    "        done = False\n",
    "        reward = -1\n",
    "\n",
    "        if self.current_state == self.goal_state:\n",
    "            done = True\n",
    "            reward = 0\n",
    "\n",
    "        return self._state_to_index(new_state), reward, done, self.steps == self.max_steps, {}\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.zeros(self.grid_size)\n",
    "        x, y = self.current_state\n",
    "        grid[x][y] = 1\n",
    "        print(grid)\n",
    "\n",
    "    def _state_to_index(self, state):\n",
    "        x, y = state\n",
    "        return x * self.grid_size[1] + y\n",
    "\n",
    "    def _index_to_state(self, index):\n",
    "        x = index // self.grid_size[1]\n",
    "        y = index % self.grid_size[1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalking(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CliffWalking, self).__init__()\n",
    "\n",
    "        self.grid_size = (4, 12)\n",
    "        self.action_space = spaces.Discrete(4)  # Up, Down, Left, Right\n",
    "        self.observation_space = spaces.Discrete(self.grid_size[0] * self.grid_size[1])\n",
    "\n",
    "        self.start_state = (3, 0)\n",
    "        self.goal_state = (3, 11)\n",
    "        self.current_state = self.start_state\n",
    "\n",
    "        self.cliff = [(3, i) for i in range(1, 11)]\n",
    "        self.steps = 0\n",
    "        self.max_steps = 500\n",
    "\n",
    "    def reset(self):\n",
    "        self.steps = 0\n",
    "        self.current_state = self.start_state\n",
    "        return self._state_to_index(self.current_state), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        x, y = self.current_state\n",
    "\n",
    "        if action == 0:  # Up\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 1:  # Down\n",
    "            x = min(self.grid_size[0] - 1, x + 1)\n",
    "        elif action == 2:  # Left\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 3:  # Right\n",
    "            y = min(self.grid_size[1] - 1, y + 1)\n",
    "\n",
    "        new_state = (x, y)\n",
    "        self.current_state = new_state\n",
    "\n",
    "        done = False\n",
    "        reward = -1\n",
    "\n",
    "        if self.current_state in self.cliff:\n",
    "            reward = -100\n",
    "            self.current_state = self.start_state\n",
    "\n",
    "        if self.current_state == self.goal_state:\n",
    "            done = True\n",
    "            reward = 0\n",
    "\n",
    "        return self._state_to_index(new_state), reward, done, self.steps == self.max_steps, {}\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.zeros(self.grid_size)\n",
    "        x, y = self.current_state\n",
    "        grid[x][y] = 1\n",
    "        print(grid)\n",
    "\n",
    "    def _state_to_index(self, state):\n",
    "        x, y = state\n",
    "        return x * self.grid_size[1] + y\n",
    "\n",
    "    def _index_to_state(self, index):\n",
    "        x = index // self.grid_size[1]\n",
    "        y = index % self.grid_size[1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6272c",
   "metadata": {},
   "source": [
    "Take a quick look over the code for these two additional classes to make sure you understand what's going on, then we'll take a look at _evaluating_ a policy. \n",
    "\n",
    "### Policy evaluation\n",
    "Recall from your lecture slides that we can evaluate a policy using the update rule $v_{k+1}(s) \\leftarrow \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_k(S_{t=1}) | S_t = s]$. **Defining some arbitrary policy (e.g. one that selects each action at random) can you write a function that would implement policy evaluation? You will need to make sure that you loop through all states, performing the update for each, *except* for the terminal state (i.e. bottom right state) -- ensure that this is always kept at a value of 0, as this is how we define a terminal state. Use `matplotlib` to plot the values of your policy. Can you define a custom policy that would be an optimal solution, and if so, what do the values look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def policy_evaluation(env, policy, gamma, theta):\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Initialize value function\n",
    "    value_function = np.zeros(num_states)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        for state in range(num_states - 1):\n",
    "            v = value_function[state]\n",
    "            new_v = 0\n",
    "\n",
    "            for action in range(num_actions):\n",
    "                next_state, reward, done = get_transitions(env, state, action)\n",
    "                new_v += policy[state][action] * (reward + gamma * value_function[next_state])\n",
    "\n",
    "            value_function[state] = new_v\n",
    "            delta = max(delta, np.abs(v - value_function[state]))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return value_function\n",
    "\n",
    "\n",
    "def get_transitions(env, state, action):\n",
    "    \"\"\"\n",
    "    You will need to use this function in your policy eval function to return the transition dynamics p(s', r|s, a).\n",
    "    It essentially just uses the same logic as in the `step` method of the gridworld class. \n",
    "    \"\"\"\n",
    "    x, y = env._state_to_position(state)\n",
    "\n",
    "    if action == 0:  # Up\n",
    "        x = max(0, x - 1)\n",
    "    elif action == 1:  # Down\n",
    "        x = min(env.grid_size - 1, x + 1)\n",
    "    elif action == 2:  # Left\n",
    "        y = max(0, y - 1)\n",
    "    elif action == 3:  # Right\n",
    "        y = min(env.grid_size - 1, y + 1)\n",
    "\n",
    "    new_state = env._position_to_state(x, y)\n",
    "\n",
    "    done = False\n",
    "    reward = -1\n",
    "\n",
    "    if state == env.goal_state:\n",
    "        done = True\n",
    "        reward = -1\n",
    "\n",
    "    return new_state, reward, done\n",
    "\n",
    "\n",
    "def plot_value_function(env, value_function):\n",
    "    num_rows, num_cols = env.grid_size, env.grid_size\n",
    "    grid = np.zeros((num_rows, num_cols))\n",
    "\n",
    "    for state in range(len(value_function)):\n",
    "        row, col = env._state_to_position(state)\n",
    "        grid[row][col] = value_function[state]\n",
    "\n",
    "    grid[-1][-1] = 0\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(grid, cmap='cool')\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            value = grid[i][j]\n",
    "            ax.text(j, i, f'{value:.2f}', ha='center', va='center', color='black')\n",
    "    ax.text(num_cols - 1, num_rows - 1, 0, ha='center', va='center', color='black')\n",
    "\n",
    "    ax.set_xticks(np.arange(num_cols))\n",
    "    ax.set_yticks(np.arange(num_rows))\n",
    "    ax.set_xticklabels(np.arange(num_cols))\n",
    "    ax.set_yticklabels(np.arange(num_rows))\n",
    "    ax.set_title('Value Function')\n",
    "    plt.colorbar(im)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "env = Gridworld(4)\n",
    "policy = {s: np.ones(env.action_space.n) / env.action_space.n for s in range(env.observation_space.n)}  # change this if you like, np.random.dirichlet can be useful\n",
    "value_function = policy_evaluation(env, policy, 1, 0.000001)\n",
    "plot_value_function(env, value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb180253",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "Now we will look to extract a policy that is approximately optimal using value iteration. **Using the pseudocode from your lecture slides, write a function that performs value iteration and will output a deterministic policy such that $\\pi(s) = \\arg\\max_a\\sum_{s', r}p(s', r|s, a)[r + \\gamma V(s')]$ (you may also want to return the value function to plot it).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma, epsilon):\n",
    "    # implement value iteration here\n",
    "    pass\n",
    "\n",
    "\n",
    "def plot_policy(env, policy):\n",
    "    \"\"\"\n",
    "    This assumes that the policy is a dictionary mapping from state -> action (action is assumed to be an integer),\n",
    "    so the policy must be deterministic.\n",
    "    \"\"\"\n",
    "    num_rows, num_cols = env.grid_size, env.grid_size\n",
    "    grid = np.zeros((num_rows, num_cols))\n",
    "    actions = ['↑', '↓', '←', '→']\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(grid, cmap='Greys')\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            if (i == num_rows - 1) and (j == num_cols - 1):\n",
    "                continue\n",
    "            state = env._position_to_state(i, j)\n",
    "            value = actions[policy[state]]\n",
    "            ax.text(j, i, value, ha='center', va='center', color='black')\n",
    "\n",
    "    ax.set_xticks(np.arange(num_cols))\n",
    "    ax.set_yticks(np.arange(num_rows))\n",
    "    ax.set_xticklabels(np.arange(num_cols))\n",
    "    ax.set_yticklabels(np.arange(num_rows))\n",
    "    ax.set_title('Policy')\n",
    "    plt.show()\n",
    "    \n",
    "policy, value_function = value_iteration(env, 1, 0.000001)\n",
    "plot_value_function(env, value_function)\n",
    "plot_policy(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8ef34",
   "metadata": {},
   "source": [
    "How do the values compare to the random policy that you evaluated earlier?\n",
    "\n",
    "Now that we have looked at some exact Dynamic Programming methods, we will move on to some Reinforcement Learning algorithms, beginning with Monte Carlo Control!\n",
    "\n",
    "--- \n",
    "\n",
    "## Monte Carlo Control\n",
    "We'll now focus on some Monte Carlo based algorithms. Monte Carlo Control is a family of RL algorithms that learns to find an optimal policy by repeatedly playing episodes of a given environment and estimating the expected return of each state-action pair based on the observed returns from those pairs in the played episodes, i.e. using a Monte Carlo approach to estimate the expected returns.\n",
    "\n",
    "Below I have provided a class that implements an algorithm based on the first visit Monte Carlo estimates of the returns. First you should read through the code to see if you can understand it for yourself, and then I will add details of each method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6220ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class MonteCarloControl:\n",
    "    def __init__(self, env, num_episodes=10000, gamma=0.99, epsilon=0.3):\n",
    "        self.env = env\n",
    "        self.test_env = copy.deepcopy(env)\n",
    "        self.num_episodes = num_episodes\n",
    "        self.gamma = gamma\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.Q = {}\n",
    "        self.N = {}\n",
    "        self.possible_states = range(env.observation_space.n)\n",
    "        self.num_states = env.observation_space.n\n",
    "        for state in self.possible_states:\n",
    "            self.Q[state] = np.random.uniform(-0.1, 0.1, self.num_actions)\n",
    "            self.N[state] = np.zeros(self.num_actions)\n",
    "            self.returns = {(s, a): [] for s in self.possible_states for a in range(self.num_actions)}\n",
    "        self.policy = self.create_random_policy()\n",
    "        self.epsilon = epsilon\n",
    "        self.test_scores = []\n",
    "\n",
    "    def create_random_policy(self):\n",
    "        policy = {}\n",
    "        for s in self.Q:\n",
    "            policy[s] = np.random.dirichlet(np.ones(self.num_actions), size=1).flatten()\n",
    "        return policy\n",
    "\n",
    "    def get_action(self, state):\n",
    "        probs = self.policy[state]\n",
    "        action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "        return action\n",
    "\n",
    "    def generate_episode(self):\n",
    "        episode = []\n",
    "        state, _ = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        return episode\n",
    "\n",
    "    def update_Q(self, episode):\n",
    "        G = 0\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = self.gamma * G + reward\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:\n",
    "                self.N[state][action] += 1\n",
    "                self.Q[state][action] += 1 / self.N[state][action] * (G - self.Q[state][action])\n",
    "                self.policy[state] = np.zeros(self.num_actions)\n",
    "                best_action = np.argmax(self.Q[state])\n",
    "                self.policy[state] = np.array([self.epsilon / self.num_actions] * self.num_actions)\n",
    "                self.policy[state][best_action] = 1 - self.epsilon + self.epsilon / self.num_actions\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            if (episode + 1) % 10 == 0:  # every 10 episodes, we test our algorithm and store the results for later\n",
    "                scores = []\n",
    "                for i in range(10):\n",
    "                    scores.append(self.generate_test_episode())\n",
    "                self.test_scores.append((episode + 1, np.mean(scores)))\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"Starting episode {episode + 1}\")\n",
    "            episode = self.generate_episode()\n",
    "            self.update_Q(episode)\n",
    "\n",
    "    def generate_test_episode(self):\n",
    "        state, _ = self.test_env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.test_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b33c2",
   "metadata": {},
   "source": [
    "In the `init` we pass in the environment, how many episodes we'd like to train for, the discount factor ($\\gamma$) and our $\\epsilon$ parameter for the soft policy. Recall that a soft policy is one which assigns probability $\\frac{\\epsilon}{|\\mathcal{A}|}$ to all non-greedy actions, and the remaining $1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}|}$ probability to the greedy value. Using the environment, we loop through all possible states and initialise our Q-table, `self.Q` and our visitation count `self.N`. We then also initialise the policy randomly using the `create_random_policy` method. \n",
    "\n",
    "Next, we have two straight forward methods: `get_action` and `generate_episode`. The former simply returns an action by sampling from the policy, whilst the latter runs through an episode similar to our above example, except here we append the `(state, action, reward)` tuple that we will use to update our Q-table. \n",
    "\n",
    "As the name suggests, the `update_Q` method is where we will use the episode information to update our Q-table. We receive an `episode` object, which is just a list of the tuples from the most recent episode. Using the list, we update Q using the first-visit Monte Carlo method, which will store the returns we saw from the *first* time we visited a state-action pair in the episode. \n",
    "\n",
    "To be more efficient, we do this dynamically using the following update: `NewEstimate = OldEstimate + 1/N * (NewValue - OldEstimate)`, where `NewValue` is the new return value we observe and `OldEstimate` is our previous estimate of the returns. Once we have update our Q-table, we then update our policy to be $\\epsilon$-soft with respect to the new Q-Values. \n",
    "\n",
    "And there we have our first RL algorithm! Let's see if it can beat the random policy that we tested with earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dfed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "env = Gridworld(6)\n",
    "monte_carlo = MonteCarloControl(env, num_episodes=1000, epsilon=0.05)\n",
    "monte_carlo.train()\n",
    "episodes = [x[0] for x in monte_carlo.test_scores]\n",
    "scores = [x[1] for x in monte_carlo.test_scores]\n",
    "plt.plot(episodes, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d2874",
   "metadata": {},
   "source": [
    "We can see that the first time we test the algorithm it takes a long time to reach the goal state (over 100 steps!), but will quickly converge to a much more reasonable policy. **Try changing the size of the grid? What do you notice happening?**\n",
    "\n",
    "---\n",
    "## Temporal Difference Learning\n",
    "We will now take a look at Temporal Difference (TD) Learning. One of the downsides of Monte Carlo learning is that we have to wait *until the end of an episode* before we receive any feedback about our actions. What if we don't want to wait this long? What if we are in a continuing task where there is no end to an episode? That is where TD Learning comes in, as rather than wait until the end of an episode to calculate the Monte Carlo returns, we simply bootstrap the remaining returns from our current estimate. The simplest example is the SARSA algorithm which makes the following update: $Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r(s, a) + \\gamma Q(s', a') - Q(s, a) \\right]$, where $a'$ is an action which we sample from our current policy $\\pi$, i.e. $a' \\sim \\pi(\\cdot|s')$. We can see that our returns are calculated by bootstrapping the value from the next state $s'$, rather than waiting for the end of the episode.\n",
    "\n",
    "Let's write some code for this and see how it compares to the Monte-Carlo method. As before, I will provide the code for you to look through to understand for yourself and then you can read my comments below on which each method is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    def __init__(self, env, alpha, gamma, epsilon, num_episodes=100000):\n",
    "        self.env = env\n",
    "        self.test_env = copy.deepcopy(env)\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.Q = {}\n",
    "        self.test_scores = []\n",
    "        self.possible_states = range(env.observation_space.n)\n",
    "        self.num_states = env.observation_space.n\n",
    "        for state in self.possible_states:\n",
    "            self.Q[state] = np.random.uniform(-0.1, 0.1, self.num_actions)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = self.env.action_space.sample()  # explore\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state])  # exploit\n",
    "        return action\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state, next_action, terminated):\n",
    "        td_target = reward + self.gamma * self.Q[next_state][next_action] * (1 - terminated)\n",
    "        td_error = td_target - self.Q[state][action]\n",
    "        self.Q[state][action] += self.alpha * td_error\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                scores = []\n",
    "                for i in range(10):\n",
    "                    scores.append(self.generate_test_episode())\n",
    "                self.test_scores.append((episode + 1, np.mean(scores)))\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"Starting episode {episode + 1}\")\n",
    "            state, _ = self.env.reset()\n",
    "            action = self.choose_action(state)\n",
    "            done = False\n",
    "            while not done:\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                next_action = self.choose_action(next_state)\n",
    "                self.update_Q(state, action, reward, next_state, next_action, terminated)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "    def generate_test_episode(self):\n",
    "        state, _ = self.test_env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action = self.choose_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.test_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        return score\n",
    "\n",
    "    def get_greedy_action(self, state):\n",
    "        return np.argmax(self.Q[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a144fd",
   "metadata": {},
   "source": [
    "The code follows the general class structure as the Monte Carlo method (in general, it is good practice to find a layout for your classes that you like and stick to it, so that any algorithms you write and publish are more readable). The only real changes are in the `train` method and the `update_Q` method. In the latter, we have just changed our update rule to be that of the SARSA algorithm. In the former, we have changed slightly the order of things -- now we take an action with the initial state, and pass in the next state to the `update_Q` method. Other than that, things are pretty much as they were!\n",
    "\n",
    "Let's see how SARSA compared to the Monte-Carlo algorithm in Gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e21d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "env = Gridworld(6)\n",
    "sarsa = SARSA(env, 0.8, 0.99, 0.05, 1000)\n",
    "sarsa.train()\n",
    "episodes = [x[0] for x in sarsa.test_scores]\n",
    "scores = [x[1] for x in sarsa.test_scores]\n",
    "plt.plot(episodes, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb3c43",
   "metadata": {},
   "source": [
    "**How does SARSA compare to monte carlo? What about as the size of the grid increases?**\n",
    "\n",
    "### Q-Learning\n",
    "We are now going to test our first off-policy algorithm, Q-Learning. This algorithm was seminal to making *deep* RL successful, and we will look at the deep variant in our next lab, but for now we will stick with the tabular version. The update rule is similar to SARSA:\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r(s, a) + \\gamma \\max_{a'}Q(s', a') - Q(s, a) \\right]$$.\n",
    "We still choose actions according to some exploratory policy, but the value we bootstrap from comes from the greedy policy. **Why is Q-Learning considered an off-policy algorithm?**\n",
    "\n",
    "To write code for Q-Learning, we need to make minimal modifications to the SARSA algorithm. **Below I've attached a child class of the SARSA algorithm and I'll leave it as an exercise to make the changes.** Once you have made the changes, run the Q-Learning algorithm and see how it performs compared to Monte-Carlo and SARSA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e7d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning(SARSA):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(QLearning, self).__init__(**kwargs)\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state, terminated):\n",
    "        # implement the Q-Learning update here  -- it should be similar to SARSA but with the Q-Learning update rule.\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                scores = []\n",
    "                for i in range(10):\n",
    "                    scores.append(self.generate_test_episode())\n",
    "                self.test_scores.append((episode + 1, np.mean(scores)))\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"Starting episode {episode + 1}\")\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                self.update_Q(state, action, reward, next_state, terminated)\n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "env = Gridworld(6)\n",
    "q_learning = QLearning(env=env, alpha=0.8, gamma=0.99, epsilon=0.05, num_episodes=1000)\n",
    "q_learning.train()\n",
    "episodes = [x[0] for x in q_learning.test_scores]\n",
    "scores = [x[1] for x in q_learning.test_scores]\n",
    "plt.plot(episodes, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35d2a8",
   "metadata": {},
   "source": [
    "How does Q-Learning compare to SARSA and Monte-Carlo? You can add the plots to the same axes and compare. You should notice that as the grid size increases, SARSA and Q-Learning start to outperform Monte-Carlo for the gridworld environment, in terms of how long the algorithm takes to run and also the performance. **Is this what you would expect and why?**\n",
    "\n",
    "In the cell below, you can run SARSA and Q-Learning on the Windy Gridworld and the Cliff Walking environments. It might be interesting for you to modify the functions from earlier that plotted the policy to work with these environments (the biggest change is that they are no longer square) and see how the behaviour of the policies learnt by each algorithm differ, particularly in the cliff walking environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cdb53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "env = WindyGridworld()\n",
    "sarsa = SARSA(env, 0.8, 0.99, 0.05, 1000)\n",
    "sarsa.train()\n",
    "sarsa_episodes = [x[0] for x in sarsa.test_scores]\n",
    "sarsa_scores = [x[1] for x in sarsa.test_scores]\n",
    "\n",
    "env = WindyGridworld()\n",
    "q_learning = QLearning(env=env, alpha=0.8, gamma=0.99, epsilon=0.05, num_episodes=1000)\n",
    "q_learning.train()\n",
    "q_learning_episodes = [x[0] for x in q_learning.test_scores]\n",
    "q_learning_scores = [x[1] for x in q_learning.test_scores]\n",
    "plt.plot(q_learning_episodes, q_learning_scores, label='q-learning')\n",
    "plt.plot(sarsa_episodes, sarsa_scores, label='sarsa')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ad1d4",
   "metadata": {},
   "source": [
    "### Double Q-Learning\n",
    "As you have seen in the lectures, Q-Learning suffers from a maximisation bias in the target: $r + \\max_{a'}Q(s', a')$. Particularly in MDPs with many actions, meaning we take a maximum over many estimates, this can lead to a positive bias. For example, we could be in a state where the true $Q(s, a)$ is zero for most of the actions, but our estimates will be distributed above and below zero. The estimates above zero lead to the positive bias. \n",
    "\n",
    "To overcome this, Double Q-Learning was introduced. In Double Q-Learning, we maintain two estimates of the Q-values, $Q_1(s, a)$ and $Q_2(s, a)$. Instead of updating both estimates using a transition from the episode, we flip a coin and if, say, the coin lands on heads, then the update is given by $Q_1(s, a) \\leftarrow Q_1(s, a) + \\alpha \\left[r + \\gamma Q_2\\left(s', \\arg\\max_{a'}Q_1(s', a')\\right) - Q_1(s, a) \\right]$.\n",
    "\n",
    "**As an exercise, I'd like you to implement Double Q-Learning and compare it to Q-Learning in a test environment**. We can find the test environment in the `utils.py` file. The example is a simple toy example (taken frmo pg. 135 of the Sutton and Barto textbook). We have an MDP with two states, $A$ and $B$. In state $A$ we can take action left or right, both of which give 0 reward. The right action will lead to an immediate terminal state, whilst the left action will lead to state $B$. From state $B$, we have $N$ actions, all of which lead to a terminal state with reward drawn from a $\\mbox{Normal}(-0.1, 1)$ distribution. The optimal action is to take the right action. Below, I will provide a Q-Learning class (essentially the same as what we implemented previously, but the init will be different as the example is custom made and not a gym environment so we need to be careful when defining the Q-Table etc.). For the Double Q-Learning, implement your own child class -- you will need to add a second Q-Table and change the update and act method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1487856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import BiasExample  # here we import the toy environment\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, env, alpha, gamma, epsilon):\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.num_actions = env.num_actions\n",
    "        self.Q = {}\n",
    "        for state in [\"A\", \"B\"]:\n",
    "            if state == \"A\":\n",
    "                self.Q[state] = np.zeros(2)  # because in state A we only have the left/right actions\n",
    "            else:\n",
    "                self.Q[state] = np.zeros(self.num_actions)  # in state B we have `num_actions` actions available.\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        num_actions = 2 if state == \"A\" else self.num_actions\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = np.random.randint(low=0, high=num_actions)\n",
    "        else:\n",
    "            max_q_val = self.Q[state].max()\n",
    "            action = np.random.choice([a for a in range(len(self.Q[state])) if self.Q[state][a] == max_q_val])\n",
    "        return action\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state, terminated):\n",
    "        td_target = reward + self.gamma * self.Q[next_state].max() * (1 - terminated)\n",
    "        td_error = td_target - self.Q[state][action]\n",
    "        self.Q[state][action] += self.alpha * td_error\n",
    "\n",
    "\n",
    "env = BiasExample(10)  # initialise the environment with 10 actions from state B\n",
    "q_learning_opts = []  # keep track of how optimally we act across runs\n",
    "for run in range(1000):\n",
    "    q_learning_opts_ep = []  # keep track of how optimal we act in this run\n",
    "    q_learning = QLearning(env, 0.1, 1, 0.1)  # after each run we need to reset the env\n",
    "    for ep in range(400):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = q_learning.choose_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            q_learning.update_Q(state, action, reward, next_state, terminated)\n",
    "            state = next_state\n",
    "        q_learning_opts_ep.append(1 - info['optimal'])  # we will make use of the info provided by the environment here, which tells us whether we acted optimally from state A or not.\n",
    "    q_learning_opts.append(q_learning_opts_ep)\n",
    "\n",
    "\n",
    "class DoubleQLearning(QLearning):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DoubleQLearning, self).__init__(**kwargs)\n",
    "        self.Q2 = {}\n",
    "        for state in [\"A\", \"B\"]:\n",
    "            if state == \"A\":\n",
    "                self.Q2[state] = np.zeros(2)  # because in state A we only have the left/right actions\n",
    "            else:\n",
    "                self.Q2[state] = np.zeros(self.num_actions)  # in state B we have `num_actions` actions available.\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        num_actions = 2 if state == \"A\" else self.num_actions\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = np.random.randint(low=0, high=num_actions)\n",
    "        else:\n",
    "            q_vals = self.Q[state] + self.Q2[state]\n",
    "            max_q_val = q_vals.max()\n",
    "            action = np.random.choice([a for a in range(len(q_vals)) if q_vals[a] == max_q_val])\n",
    "        return action\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state, terminated):\n",
    "        # implement the double Q-Learning update here. you can use np.random.randint for simulating a coin toss.\n",
    "        pass \n",
    "\n",
    "\n",
    "double_opts = []  # keep track of how optimally we act across runs\n",
    "for run in range(1000):\n",
    "    double_opts_ep = []  # keep track of how optimal we act in this run\n",
    "    double_q_learning = DoubleQLearning(env=env, alpha=0.1, gamma=1, epsilon=0.1)  # after each run we need to reset the env\n",
    "    for ep in range(400):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = double_q_learning.choose_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            double_q_learning.update_Q(state, action, reward, next_state, terminated)\n",
    "            state = next_state\n",
    "        double_opts_ep.append(1 - info['optimal'])  # we will make use of the info provided by the environment here, which tells us whether we acted optimally from state A or not.\n",
    "    double_opts.append(double_opts_ep)\n",
    "\n",
    "plt.plot(np.array(q_learning_opts).mean(axis=0), label=\"Q-Learning\")\n",
    "plt.plot(np.array(double_opts).mean(axis=0), label=\"Double Q-Learning\")\n",
    "plt.axhline(y=0.05, color='black', linestyle='dashed', label='optimal')\n",
    "plt.legend()\n",
    "plt.ylabel(\"%age of time left action chosen\")\n",
    "plt.xlabel(\"Number of episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381acae1",
   "metadata": {},
   "source": [
    "In the above cell you should implement the Double Q-Learning code and then run a similar experiment, with the same hyperparameters, and add the Double Q-Learning performance to the plot. **How does it compare to Q-Learning? Does changing the hyperparameters help at all? How do the performances compare as you increase the number of actions?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
